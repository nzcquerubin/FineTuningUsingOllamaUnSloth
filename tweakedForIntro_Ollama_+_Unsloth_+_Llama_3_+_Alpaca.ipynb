{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://ollama.com/\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/ollama.png\" height=\"44\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).\n",
        "\n",
        "You will learn how to do [data prep](#Data) and import a CSV, how to [train](#Train), how to [run the model](#Inference), & [how to export to Ollama!](#Ollama)\n",
        "\n",
        "[Unsloth](https://github.com/unslothai/unsloth) now allows you to automatically finetune and create a [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md), and export to [Ollama](https://ollama.com/)! This makes finetuning much easier and provides a seamless workflow from `Unsloth` to `Ollama`!\n",
        "\n",
        "**[NEW]** We now allow uploading CSVs, Excel files - try it [here](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) by using the Titanic dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth \"xformers==0.0.28.post2\"\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your google drive in order to access the dataset you'll use to fine tune your AI"
      ],
      "metadata": {
        "id": "A3nktVHaY8qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jRkAXqXAhTpe",
        "outputId": "b9594755-0c3f-40ba-fdde-8555cac9f085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.\n",
        "* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "c100c25705df4f6eb7d657f16df1b868",
            "03359f95bbfa43fd9fec78ea2709f7f5",
            "3966320b40ea46af9741359dee1d8d60",
            "f9012d900c8144d58e896f2673740f78",
            "2a0a75b4bd204061a96e5bfa12386592",
            "ba81b7c52cc84fd5a1fca86783752870",
            "b07ca5f1cb174285bf96e08ef5c646df",
            "9967851c20dc4aa6a05837aa97b9549b",
            "1a66c9e6789e4a438bcdc0d49f2cd5da",
            "713dd636427141f39b7be172394225dd",
            "3352793db841415290c454c001020fa8",
            "3ff666ac20a149ae97c71950fddea7a7",
            "8918f484ff4844019ac47ccb5d3ee133",
            "f4762d1bd85b434b9fc97be6215abfaf",
            "52db8a52b5284ead9e8435ebfa4ee504",
            "8d804289d08445ffb3dc8781d593198b",
            "bda7410491c14a8c8c5b5bbec001d44f",
            "b8ad645caed545e0b7352162c2f677de",
            "83c5eac75f58478294efca43a39d33f5",
            "b43f9510dcdc4c368a15d8ade9f800cd",
            "1afc144ae889468eafe150f9f2d23505",
            "219bb52cb55a4ee6b62bbe71ad842007"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "f8c5c213-0d94-42a1-8d8d-f326ce61a74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c100c25705df4f6eb7d657f16df1b868"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ff666ac20a149ae97c71950fddea7a7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3.2-1B-Instruct-bnb-4bit\", #modeul used from browsing huggingface.co\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be02f179-ce13-435a-c07c-7b05fe84eedb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.12.12 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [vicgalle](https://huggingface.co/datasets/vicgalle/alpaca-gpt4), which is a version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html) generated from GPT4. You can replace this code section with your own data prep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvOPfPnet76H",
        "outputId": "a75f00f9-f79d-46eb-bc83-d7f671802adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['instruction', 'input', 'output'],\n",
            "    num_rows: 144\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "#dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/template/template_dataset.json\", split=\"train\") # Modified from the original code, grabs the dataset from the google drive. Edit to match path for your case\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/modelv2/model/dataset.json\", split=\"train\") # Modified from the original code, grabs the dataset from the google drive. Edit to match path for your case\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4_dG-m0Cz4"
      },
      "source": [
        "One issue is this dataset has multiple columns. For `Ollama` and `llama.cpp` to function like a custom `ChatGPT` Chatbot, we must only have 2 columns - an `instruction` and an `output` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTQR4jrDMcJf",
        "outputId": "1c62d995-03f3-450b-cf10-08fa337aa75c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['instruction', 'input', 'output']\n"
          ]
        }
      ],
      "source": [
        "print(dataset.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwEbRFl0Mf3E"
      },
      "source": [
        "To solve this, we shall do the following:\n",
        "* Merge all columns into 1 instruction prompt.\n",
        "* Remember LLMs are text predictors, so we can customize the instruction to anything we like!\n",
        "* Use the `to_sharegpt` function to do this column merging process!\n",
        "\n",
        "For example below in our [Titanic CSV finetuning notebook](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing), we merged multiple columns in 1 prompt:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Merge.png\" height=\"100\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w61VJ7rQM8jT"
      },
      "source": [
        "To merge multiple columns into 1, use `merged_prompt`.\n",
        "* Enclose all columns in curly braces `{}`.\n",
        "* Optional text must be enclused in `[[]]`. For example if the column \"Pclass\" is empty, the merging function will not show the text and skp this. This is useful for datasets with missing values.\n",
        "* You can select every column, or a few!\n",
        "* Select the output or target / prediction column in `output_column_name`. For the Alpaca dataset, this will be `output`.\n",
        "\n",
        "To make the finetune handle multiple turns (like in ChatGPT), we have to create a \"fake\" dataset with multiple turns - we use `conversation_extension` to randomnly select some conversations from the dataset, and pack them together into 1 conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jZxeGSeX0CR8"
      },
      "outputs": [],
      "source": [
        "from unsloth import to_sharegpt\n",
        "dataset = to_sharegpt(\n",
        "    dataset,\n",
        "    merged_prompt = \"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
        "    output_column_name = \"output\",\n",
        "    conversation_extension = 3, # Select more to handle longer conversations\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kh90vpD1jYJ"
      },
      "source": [
        "Finally use `standardize_sharegpt` to fix up the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZPwDXBvP1g8S"
      },
      "outputs": [],
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GThrcKACxTe2"
      },
      "source": [
        "### Customizable Chat Templates\n",
        "\n",
        "You also need to specify a chat template. Previously, you could use the Alpaca format as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MVBanRIJRAcQ"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTzZ5oZrxkFz"
      },
      "source": [
        "Now, you have to use `{INPUT}` for the instruction and `{OUTPUT}` for the response.\n",
        "\n",
        "We also allow you to use an optional `{SYSTEM}` field. This is useful for Ollama when you want to use a custom system prompt (also like in ChatGPT).\n",
        "\n",
        "You can also not put a `{SYSTEM}` field, and just put plain text.\n",
        "\n",
        "```python\n",
        "chat_template = \"\"\"{SYSTEM}\n",
        "USER: {INPUT}\n",
        "ASSISTANT: {OUTPUT}\"\"\"\n",
        "```\n",
        "\n",
        "Use below if you want to use the Llama-3 prompt format. You must use the `instruct` and not the `base` model if you use this!\n",
        "```python\n",
        "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{OUTPUT}<|eot_id|>\"\"\"\n",
        "```\n",
        "\n",
        "For the ChatML format:\n",
        "```python\n",
        "chat_template = \"\"\"<|im_start|>system\n",
        "{SYSTEM}<|im_end|>\n",
        "<|im_start|>user\n",
        "{INPUT}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{OUTPUT}<|im_end|>\"\"\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK-_ncj-RCNy"
      },
      "source": [
        "The issue is the Alpaca format has 3 fields, whilst OpenAI style chatbots must only use 2 fields (instruction and response). That's why we used the `to_sharegpt` function to merge these columns into 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c1879cf855de407f958db716d840a6b7",
            "d7d96f09dfbe4c6d99fc4a5f6d3254ad",
            "93ed2d565e334db78ecd697af6d81a91",
            "80f6acbb5a6c43c99787f7aa815c56e5",
            "f420101a83c54c739b69bd3860fb3723",
            "90825c73725744faa03b4282d88ebf41",
            "9b9f96d2347641fa8edd757bb1de1662",
            "d0aa8c0ce8bf4a3d97e71497af6a0ed4",
            "c068be63f2974c86b9f2882a5d3edecb",
            "1ca6c0151bc944d8b0d2bde1569d62ad",
            "0f124d28fdc94ab8b41eef5208c1f495"
          ]
        },
        "id": "JOGaZf1sdLlr",
        "outputId": "93c4186c-1e33-4dcb-c5db-1266e305f7f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: We automatically added an EOS token to stop endless generations.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/144 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1879cf855de407f958db716d840a6b7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
        "\n",
        "### Instruction:\n",
        "{INPUT}\n",
        "\n",
        "### Response:\n",
        "{OUTPUT}\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        "    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a3eec9f5f98e4c0b8de8f9364cdf9da5",
            "80cf61343eb948579c28f846d543d209",
            "bbad6be4094547878780cee2e8cc288f",
            "cf1591a58cbc412c97f095e6cd5c8ff5",
            "de7b7e0e15004b81ba15d57e2d58fe0f",
            "4890ec236f0e4764b3ab72125acb59df",
            "cf96f214524d44848b2cbdfb7640c903",
            "ab9215ae18d54a2b9e098d4652024f40",
            "6a3800e52e3e49ceb4f102cb547c8eee",
            "f03034c363f944f0aeb0bd2fd49dec75",
            "7ef18d29a0b54c859e26bd044a2fdd72"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "a2d8f099-5610-4cdc-d6ba-cc3ce0774092"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/144 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3eec9f5f98e4c0b8de8f9364cdf9da5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        # num_train_epochs = 1, # For longer training runs!\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "b056d05d-889a-4b27-ac5b-5c1715241d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "1.148 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "f530ae02-4060-4646-98a0-87141286f483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 144 | Num Epochs = 4\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 11,272,192\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:50, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.921300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.213400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.795400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.597500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.727400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.787500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.572800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.462500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.202500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.151300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.922700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.608600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.629600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.720100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.820400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.678100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.353300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.154500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.088900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.173200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.893200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.307300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.244100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.804500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.961800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.644000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.027400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.573100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.801100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.577400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.711100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.926800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.466100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.058000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.245200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.718400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.540400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.524100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.416500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.316900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.898800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.175100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.209600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.185000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.326800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.118600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.221200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.250800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.217000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.804200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.726600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.216300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.471400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.212000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.244800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "970c1bf1-8eb4-4d6a-b7f0-466081a6edb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115.1846 seconds used for training.\n",
            "1.92 minutes used for training.\n",
            "Peak reserved memory = 2.217 GB.\n",
            "Peak reserved memory for training = 1.069 GB.\n",
            "Peak reserved memory % of max memory = 15.033 %.\n",
            "Peak reserved memory for training % of max memory = 7.248 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! Unsloth makes inference natively 2x faster as well! You should use prompts which are similar to the ones you had finetuned on, otherwise you might get bad results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "3aac500c-465b-4874-d0ea-7426e0ec29b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Map√∫a's School of Information Technology (SOIT) is one of the University's strengths. The program is designed to train the next generation of tech leaders, preparing you for a career in teaching and learning information technology. The course covers various specializations such as Computer Network Security, Computer Science, Data Science, Enterprise Resource Planning, IT Audit, IT Service Management, Information Systems Security, Information Technology, and Software Development. The program is accredited by ABET‚Äôs Computing Accreditation Commission and is classified as ‚ÄúA‚Äù by CUNY‚Äôs Accreditation Commission of College Education. SOIT is also ranked 2 stars by Quacquarelli\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": \"Give me an overview of Mapua's SOIT?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        "Since we created an actual chatbot, you can also do longer conversations by manually adding alternating conversations between the user and assistant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcbFUWEyQVaE",
        "outputId": "b9c41ace-0de3-4eed-bb08-ce61a86e5f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Torre Cipriani is the tallest tower in the Philippines.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [                         # Change below!\n",
        "    {\"role\": \"user\",      \"content\": \"Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The fibonacci sequence continues as 13, 21, 34, 55 and 89.\"},\n",
        "    {\"role\": \"user\",      \"content\": \"What is France's tallest tower called?\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "b660be65-d81c-42e2-bfcf-d6af674addff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "1828c6d2-b867-4462-b833-aab8783a309a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are no sequences in the given list.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass\n",
        "\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": \"Describe anything special about a sequence. Your input is 1, 1, 2, 3, 5, 8,\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOFzC441vCtq"
      },
      "source": [
        "<a name=\"Ollama\"></a>\n",
        "### Ollama Support\n",
        "\n",
        "[Unsloth](https://github.com/unslothai/unsloth) now allows you to automatically finetune and create a [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md), and export to [Ollama](https://ollama.com/)! This makes finetuning much easier and provides a seamless workflow from `Unsloth` to `Ollama`!\n",
        "\n",
        "Let's first install `Ollama`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUxcyP_UfeLl",
        "outputId": "c6af581b-0129-48f2-bc2c-e34d78e7238e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/ggerganov/llama.cpp\n",
        "!make clean -C llama.cpp\n",
        "!make all -j -C llama.cpp\n",
        "!pip install gguf protobuf\n",
        "# run below if you encounter the following error\n",
        "# Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist.\n",
        "# But we expect this file to exist! Maybe the llama.cpp developers changed the name?\n",
        "!(cd llama.cpp; cmake -B build;cmake --build build --config Release)\n",
        "!cp /content/llama.cpp/build/bin/llama-quantize /content/llama.cpp"
      ],
      "metadata": {
        "id": "A4kTvevdlXPT",
        "outputId": "3f964706-4631-4e34-a81d-7037864dc81f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n",
            "make: Entering directory '/content/llama.cpp'\n",
            "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md.  Stop.\n",
            "make: Leaving directory '/content/llama.cpp'\n",
            "make: Entering directory '/content/llama.cpp'\n",
            "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md.  Stop.\n",
            "make: Leaving directory '/content/llama.cpp'\n",
            "Requirement already satisfied: gguf in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from gguf) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from gguf) (6.0.2)\n",
            "Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.10/dist-packages (from gguf) (0.2.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from gguf) (4.67.1)\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- Including CPU backend\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Configuring done (0.2s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  4%] Built target ggml-base\n",
            "[  9%] Built target ggml-cpu\n",
            "[ 10%] Built target ggml\n",
            "[ 19%] Built target llama\n",
            "[ 20%] Built target build_info\n",
            "[ 24%] Built target common\n",
            "[ 25%] Built target test-tokenizer-0\n",
            "[ 27%] Built target test-sampling\n",
            "[ 29%] Built target test-grammar-parser\n",
            "[ 30%] Built target test-grammar-integration\n",
            "[ 32%] Built target test-llama-grammar\n",
            "[ 33%] Built target test-json-schema-to-grammar\n",
            "[ 34%] Built target test-tokenizer-1-bpe\n",
            "[ 35%] Built target test-tokenizer-1-spm\n",
            "[ 36%] Built target test-log\n",
            "[ 37%] Built target test-arg-parser\n",
            "[ 38%] Built target test-chat-template\n",
            "[ 40%] Built target test-gguf\n",
            "[ 41%] Built target test-backend-ops\n",
            "[ 43%] Built target test-model-load-cancel\n",
            "[ 45%] Built target test-autorelease\n",
            "[ 47%] Built target test-barrier\n",
            "[ 48%] Built target test-quantize-fns\n",
            "[ 50%] Built target test-quantize-perf\n",
            "[ 51%] Built target test-rope\n",
            "[ 52%] Built target test-c\n",
            "[ 53%] Built target llama-batched-bench\n",
            "[ 54%] Built target llama-batched\n",
            "[ 55%] Built target llama-embedding\n",
            "[ 56%] Built target llama-eval-callback\n",
            "[ 57%] Built target llama-gbnf-validator\n",
            "[ 58%] Built target sha256\n",
            "[ 59%] Built target xxhash\n",
            "[ 59%] Built target sha1\n",
            "[ 60%] Built target llama-gguf-hash\n",
            "[ 61%] Built target llama-gguf-split\n",
            "[ 62%] Built target llama-gguf\n",
            "[ 63%] Built target llama-gritlm\n",
            "[ 64%] Built target llama-imatrix\n",
            "[ 65%] Built target llama-infill\n",
            "[ 66%] Built target llama-bench\n",
            "[ 67%] Built target llama-lookahead\n",
            "[ 68%] Built target llama-lookup\n",
            "[ 69%] Built target llama-lookup-create\n",
            "[ 70%] Built target llama-lookup-merge\n",
            "[ 71%] Built target llama-lookup-stats\n",
            "[ 72%] Built target llama-cli\n",
            "[ 74%] Built target llama-parallel\n",
            "[ 75%] Built target llama-passkey\n",
            "[ 76%] Built target llama-perplexity\n",
            "[ 77%] Built target llama-quantize\n",
            "[ 78%] Built target llama-retrieval\n",
            "[ 80%] Built target llama-server\n",
            "[ 81%] Built target llama-save-load-state\n",
            "[ 82%] Built target llama-run\n",
            "[ 83%] Built target llama-simple\n",
            "[ 84%] Built target llama-simple-chat\n",
            "[ 85%] Built target llama-speculative\n",
            "[ 86%] Built target llama-speculative-simple\n",
            "[ 87%] Built target llama-tokenize\n",
            "[ 88%] Built target llama-tts\n",
            "[ 89%] Built target llama-gen-docs\n",
            "[ 90%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 91%] Built target llama-cvector-generator\n",
            "[ 92%] Built target llama-export-lora\n",
            "[ 93%] Built target llama-quantize-stats\n",
            "[ 94%] Built target llava\n",
            "[ 95%] Built target llava_static\n",
            "[ 95%] Built target llava_shared\n",
            "[ 96%] Built target llama-llava-cli\n",
            "[ 97%] Built target llama-minicpmv-cli\n",
            "[ 98%] Built target llama-qwen2vl-cli\n",
            "[ 99%] Built target llama-vdot\n",
            "[100%] Built target llama-q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "Next, we shall save the model to GGUF / llama.cpp\n",
        "\n",
        "We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "We also support saving to multiple GGUF options in a list fashion! This can speed things up by 10 minutes or more if you want multiple export formats!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqfebeAdT073",
        "outputId": "dff56578-bd4d-4e13-b4af-ac39834a0c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n",
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 1.0G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 6.66 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 27.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model/pytorch_model.bin...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at model into f16 GGUF format.\n",
            "The output location will be /content/model/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {2048, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {2048, 512}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {2048, 2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {2048, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 2048}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {2048}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "2025-01-07 07:31:41.258327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-07 07:31:41.284443: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-07 07:31:41.294592: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-07 07:31:43.307324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting special token type pad to 128004\n",
            "INFO:gguf.vocab:Setting chat_template to {{ 'Below are some instructions that describe some tasks. Write responses that appropriately complete each request.' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '\n",
            "\n",
            "### Instruction:\n",
            "' + message['content'] }}{% elif message['role'] == 'assistant' %}{{ '\n",
            "\n",
            "### Response:\n",
            "' + message['content'] + '<|eot_id|>' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\n",
            "\n",
            "### Response:\n",
            "' }}{% endif %}\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/model/unsloth.F16.gguf: n_tensors = 147, total_size = 2.5G\n",
            "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.47G/2.47G [00:32<00:00, 76.2Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4433 (a4dd4900)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 147 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 1B Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 1B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 16\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 64\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 64\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{ 'Below are some instructions that ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   34 tensors\n",
            "llama_model_loader: - type  f16:  113 tensors\n",
            "[   1/ 147]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   2/ 147]                    rope_freqs.weight - [   32,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 147]                    token_embd.weight - [ 2048, 128256,     1,     1], type =    f16, converting to q6_K .. size =   501.00 MiB ->   205.49 MiB\n",
            "[   4/ 147]                  blk.0.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[   5/ 147]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[   6/ 147]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   7/ 147]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   8/ 147]                  blk.0.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[   9/ 147]                blk.0.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  10/ 147]                blk.0.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  11/ 147]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  12/ 147]                  blk.0.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  13/ 147]                  blk.1.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  14/ 147]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  15/ 147]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  16/ 147]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  17/ 147]                  blk.1.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  18/ 147]                blk.1.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  19/ 147]                blk.1.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  20/ 147]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  21/ 147]                  blk.1.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  22/ 147]                  blk.2.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  23/ 147]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  24/ 147]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  25/ 147]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  26/ 147]                  blk.2.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  27/ 147]                blk.2.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  28/ 147]                blk.2.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  29/ 147]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  30/ 147]                  blk.2.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  31/ 147]                  blk.3.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  32/ 147]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  33/ 147]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  34/ 147]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  35/ 147]                  blk.3.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  36/ 147]                blk.3.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  37/ 147]                blk.3.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  38/ 147]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  39/ 147]                  blk.3.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  40/ 147]                  blk.4.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  41/ 147]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  42/ 147]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  43/ 147]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  44/ 147]                  blk.4.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  45/ 147]                blk.4.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  46/ 147]                blk.4.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  47/ 147]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  48/ 147]                  blk.4.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  49/ 147]                  blk.5.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  50/ 147]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  51/ 147]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  52/ 147]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  53/ 147]                  blk.5.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  54/ 147]                blk.5.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  55/ 147]                blk.5.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  56/ 147]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  57/ 147]                  blk.5.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  58/ 147]                  blk.6.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  59/ 147]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  60/ 147]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  61/ 147]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  62/ 147]                  blk.6.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  63/ 147]                blk.6.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  64/ 147]                blk.6.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  65/ 147]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  66/ 147]                  blk.6.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  67/ 147]                  blk.7.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  68/ 147]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  69/ 147]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  70/ 147]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  71/ 147]                  blk.7.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  72/ 147]                blk.7.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[  73/ 147]                blk.7.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  74/ 147]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  75/ 147]                  blk.7.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  76/ 147]                  blk.8.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  77/ 147]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  78/ 147]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  79/ 147]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  80/ 147]                  blk.8.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  81/ 147]                blk.8.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  82/ 147]                blk.8.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  83/ 147]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  84/ 147]                  blk.8.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  85/ 147]                  blk.9.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  86/ 147]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  87/ 147]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  88/ 147]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  89/ 147]                  blk.9.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  90/ 147]                blk.9.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  91/ 147]                blk.9.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  92/ 147]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  93/ 147]                  blk.9.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  94/ 147]                 blk.10.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[  95/ 147]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[  96/ 147]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  97/ 147]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  98/ 147]                 blk.10.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[  99/ 147]               blk.10.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 100/ 147]               blk.10.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 101/ 147]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 102/ 147]                 blk.10.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 103/ 147]                 blk.11.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 104/ 147]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 105/ 147]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 106/ 147]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 107/ 147]                 blk.11.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 108/ 147]               blk.11.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 109/ 147]               blk.11.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 110/ 147]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 111/ 147]                 blk.11.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 112/ 147]                 blk.12.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 113/ 147]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 114/ 147]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 115/ 147]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 116/ 147]                 blk.12.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 117/ 147]               blk.12.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 118/ 147]               blk.12.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 119/ 147]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 120/ 147]                 blk.12.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 121/ 147]                 blk.13.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 122/ 147]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 123/ 147]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 124/ 147]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 125/ 147]                 blk.13.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 126/ 147]               blk.13.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 127/ 147]               blk.13.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 128/ 147]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 129/ 147]                 blk.13.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 130/ 147]                 blk.14.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 131/ 147]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 132/ 147]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 133/ 147]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 134/ 147]                 blk.14.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 135/ 147]               blk.14.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 136/ 147]               blk.14.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 137/ 147]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 138/ 147]                 blk.14.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 139/ 147]                 blk.15.attn_k.weight - [ 2048,   512,     1,     1], type =    f16, converting to q4_K .. size =     2.00 MiB ->     0.56 MiB\n",
            "[ 140/ 147]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 141/ 147]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 142/ 147]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 143/ 147]                 blk.15.attn_v.weight - [ 2048,   512,     1,     1], type =    f16, converting to q6_K .. size =     2.00 MiB ->     0.82 MiB\n",
            "[ 144/ 147]               blk.15.ffn_down.weight - [ 8192,  2048,     1,     1], type =    f16, converting to q6_K .. size =    32.00 MiB ->    13.12 MiB\n",
            "[ 145/ 147]               blk.15.ffn_gate.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 146/ 147]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
            "[ 147/ 147]                 blk.15.ffn_up.weight - [ 2048,  8192,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "llama_model_quantize_impl: model size  =  2357.26 MB\n",
            "llama_model_quantize_impl: quant size  =   762.81 MB\n",
            "\n",
            "main: quantize time = 140138.23 ms\n",
            "main:    total time = 140138.23 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Conversion completed! Output location: /content/model/unsloth.Q4_K_M.gguf\n",
            "Unsloth: Saved Ollama Modelfile to model/Modelfile\n"
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7lk6l0CuPXS"
      },
      "source": [
        "We use `subprocess` to start `Ollama` up in a non blocking fashion! In your own desktop, you can simply open up a new `terminal` and type `ollama serve`, but in Colab, we have to use this hack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mcP9omF_tN7Q"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Wait for a few seconds for Ollama to load!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md3PExRLRhOc"
      },
      "source": [
        "`Ollama` needs a `Modelfile`, which specifies the model's prompt format. Let's print Unsloth's auto generated one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "h82vfNigRhiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c533282-0385-4cff-f00e-029ed11a0fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FROM {__FILE_LOCATION__}\n",
            "\n",
            "TEMPLATE \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.{{ if .Prompt }}\n",
            "\n",
            "### Instruction:\n",
            "{{ .Prompt }}{{ end }}\n",
            "\n",
            "### Response:\n",
            "{{ .Response }}<|eot_id|>\"\"\"\n",
            "\n",
            "PARAMETER stop \"<|python_tag|>\"\n",
            "PARAMETER stop \"<|end_of_text|>\"\n",
            "PARAMETER stop \"<|finetune_right_pad_id|>\"\n",
            "PARAMETER stop \"<|start_header_id|>\"\n",
            "PARAMETER stop \"<|end_header_id|>\"\n",
            "PARAMETER stop \"<|eom_id|>\"\n",
            "PARAMETER stop \"<|eot_id|>\"\n",
            "PARAMETER stop \"<|reserved_special_token_\"\n",
            "PARAMETER temperature 1.5\n",
            "PARAMETER min_p 0.1\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6cipBJBudxv"
      },
      "source": [
        "We now will create an `Ollama` model called `unsloth_model` using the `Modelfile` which we auto generated!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SDTUJv_QiaVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720ae40b-d8ba-494c-d98b-51c07be6103f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25ltransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 2% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 4% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 5% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 6% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 8% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 10% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 11% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 12% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 14% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 15% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 16% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 18% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 20% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 21% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 22% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 24% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 25% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 26% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 28% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 30% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 31% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 32% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 34% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 35% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 36% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 38% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 40% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 41% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 42% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 44% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 45% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 46% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 48% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 50% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 51% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 52% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 53% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 55% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 56% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 57% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 59% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 61% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 62% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 63% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 65% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 66% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 67% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 69% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 70% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 71% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 73% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 75% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 76% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 77% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 79% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 81% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 82% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 83% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 85% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 86% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 87% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 89% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 91% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 92% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 93% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 95% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 96% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 97% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 99% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% \n",
            "using existing layer sha256:40e972089a265b8a9fd969634437e7ed8fc3d8138a837da7ded0006ab600e8f1 \n",
            "creating new layer sha256:a171a8dc44d851d6f1a33ddcbe8af761f9c903bb55706d775ed46d0e1e6cccfe \n",
            "creating new layer sha256:7ff67005831890ba806524e5dad8816a9fdd3cbbbbab107c6ea823877f588809 \n",
            "creating new layer sha256:28872bdfeb9264f38ff672cae720d34e08b29896f26f8159b71db95062fb96e0 \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KSoKTKQukba"
      },
      "source": [
        "And now we can do inference on it via `Ollama`!\n",
        "\n",
        "You can also upload to `Ollama` and try the `Ollama` Desktop app by heading to https://www.ollama.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rkp0uMrNpYaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8506d956-067b-4150-d7d0-c68e40a1ce0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:06.888826664Z\",\"message\":{\"role\":\"assistant\",\"content\":\"Map\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:06.906328393Z\",\"message\":{\"role\":\"assistant\",\"content\":\"√∫a\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:06.92474223Z\",\"message\":{\"role\":\"assistant\",\"content\":\" University\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:06.941913949Z\",\"message\":{\"role\":\"assistant\",\"content\":\"'s\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:06.959272952Z\",\"message\":{\"role\":\"assistant\",\"content\":\" School\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:06.976172808Z\",\"message\":{\"role\":\"assistant\",\"content\":\" of\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:06.992058713Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Information\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.00793668Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Technology\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.025295093Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.043401291Z\",\"message\":{\"role\":\"assistant\",\"content\":\" in\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.059751397Z\",\"message\":{\"role\":\"assistant\",\"content\":\" partnership\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.075653922Z\",\"message\":{\"role\":\"assistant\",\"content\":\" with\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.091475013Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Intel\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.107362127Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.123171778Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.139062452Z\",\"message\":{\"role\":\"assistant\",\"content\":\" also\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.154902052Z\",\"message\":{\"role\":\"assistant\",\"content\":\" accredited\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.170853692Z\",\"message\":{\"role\":\"assistant\",\"content\":\" by\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.186646531Z\",\"message\":{\"role\":\"assistant\",\"content\":\" AB\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.199394585Z\",\"message\":{\"role\":\"assistant\",\"content\":\"ET\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.213662108Z\",\"message\":{\"role\":\"assistant\",\"content\":\"‚Äôs\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.22664282Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Computing\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.239788344Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Accred\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.252541265Z\",\"message\":{\"role\":\"assistant\",\"content\":\"itation\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.265260205Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Commission\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.278003829Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.290905519Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Visit\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.304825899Z\",\"message\":{\"role\":\"assistant\",\"content\":\" your\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.319230816Z\",\"message\":{\"role\":\"assistant\",\"content\":\" dean\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.332035741Z\",\"message\":{\"role\":\"assistant\",\"content\":\"‚Äôs\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.344793183Z\",\"message\":{\"role\":\"assistant\",\"content\":\" and\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.357503798Z\",\"message\":{\"role\":\"assistant\",\"content\":\" chair\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.370292551Z\",\"message\":{\"role\":\"assistant\",\"content\":\"person\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.38454085Z\",\"message\":{\"role\":\"assistant\",\"content\":\"‚Äôs\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.398592002Z\",\"message\":{\"role\":\"assistant\",\"content\":\" office\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.412616224Z\",\"message\":{\"role\":\"assistant\",\"content\":\" to\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.429404465Z\",\"message\":{\"role\":\"assistant\",\"content\":\" learn\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.443950197Z\",\"message\":{\"role\":\"assistant\",\"content\":\" more\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.457704869Z\",\"message\":{\"role\":\"assistant\",\"content\":\" about\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.472908107Z\",\"message\":{\"role\":\"assistant\",\"content\":\" the\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.486982888Z\",\"message\":{\"role\":\"assistant\",\"content\":\" program\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.503768989Z\",\"message\":{\"role\":\"assistant\",\"content\":\" and\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.518486042Z\",\"message\":{\"role\":\"assistant\",\"content\":\" how\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.532853265Z\",\"message\":{\"role\":\"assistant\",\"content\":\" to\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.547128533Z\",\"message\":{\"role\":\"assistant\",\"content\":\" apply\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.561079541Z\",\"message\":{\"role\":\"assistant\",\"content\":\".\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2025-01-07T07:37:07.575279593Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":15109175450,\"load_duration\":13912823827,\"prompt_eval_count\":29,\"prompt_eval_duration\":494000000,\"eval_count\":47,\"eval_duration\":693000000}\n"
          ]
        }
      ],
      "source": [
        "!curl http://localhost:11434/api/chat -d '{ \\\n",
        "    \"model\": \"unsloth_model\", \\\n",
        "    \"messages\": [ \\\n",
        "        { \"role\": \"user\", \"content\": \"Where is Mapua?\" } \\\n",
        "    ] \\\n",
        "    }'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT interactive mode\n",
        "\n",
        "### ‚≠ê To run the finetuned model like in a ChatGPT style interface, first click the **| >_ |** button.\n",
        "![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Where_Terminal.png)\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "\n",
        "### ‚≠ê Then, type `ollama run unsloth_model`\n",
        "\n",
        "![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Terminal_Type.png)\n",
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "### ‚≠ê And you have a CHatGPT style assistant!\n",
        "\n",
        "### Type any question you like and press `ENTER`. If you want to exit, hit `CTRL + D`\n",
        "![](https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/Assistant.png)"
      ],
      "metadata": {
        "id": "XnMbhp7KsKhr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Try our [Ollama CSV notebook](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) to upload CSVs for finetuning!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://ollama.com/\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/ollama.png\" height=\"44\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c100c25705df4f6eb7d657f16df1b868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03359f95bbfa43fd9fec78ea2709f7f5",
              "IPY_MODEL_3966320b40ea46af9741359dee1d8d60",
              "IPY_MODEL_f9012d900c8144d58e896f2673740f78"
            ],
            "layout": "IPY_MODEL_2a0a75b4bd204061a96e5bfa12386592"
          }
        },
        "03359f95bbfa43fd9fec78ea2709f7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba81b7c52cc84fd5a1fca86783752870",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b07ca5f1cb174285bf96e08ef5c646df",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "3966320b40ea46af9741359dee1d8d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9967851c20dc4aa6a05837aa97b9549b",
            "max": 1027676737,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a66c9e6789e4a438bcdc0d49f2cd5da",
            "value": 1027676639
          }
        },
        "f9012d900c8144d58e896f2673740f78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_713dd636427141f39b7be172394225dd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3352793db841415290c454c001020fa8",
            "value": "‚Äá1.03G/1.03G‚Äá[00:10&lt;00:00,‚Äá381MB/s]"
          }
        },
        "2a0a75b4bd204061a96e5bfa12386592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba81b7c52cc84fd5a1fca86783752870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07ca5f1cb174285bf96e08ef5c646df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9967851c20dc4aa6a05837aa97b9549b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a66c9e6789e4a438bcdc0d49f2cd5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "713dd636427141f39b7be172394225dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3352793db841415290c454c001020fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ff666ac20a149ae97c71950fddea7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8918f484ff4844019ac47ccb5d3ee133",
              "IPY_MODEL_f4762d1bd85b434b9fc97be6215abfaf",
              "IPY_MODEL_52db8a52b5284ead9e8435ebfa4ee504"
            ],
            "layout": "IPY_MODEL_8d804289d08445ffb3dc8781d593198b"
          }
        },
        "8918f484ff4844019ac47ccb5d3ee133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bda7410491c14a8c8c5b5bbec001d44f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b8ad645caed545e0b7352162c2f677de",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "f4762d1bd85b434b9fc97be6215abfaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83c5eac75f58478294efca43a39d33f5",
            "max": 184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b43f9510dcdc4c368a15d8ade9f800cd",
            "value": 184
          }
        },
        "52db8a52b5284ead9e8435ebfa4ee504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1afc144ae889468eafe150f9f2d23505",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_219bb52cb55a4ee6b62bbe71ad842007",
            "value": "‚Äá184/184‚Äá[00:00&lt;00:00,‚Äá14.7kB/s]"
          }
        },
        "8d804289d08445ffb3dc8781d593198b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bda7410491c14a8c8c5b5bbec001d44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8ad645caed545e0b7352162c2f677de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83c5eac75f58478294efca43a39d33f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b43f9510dcdc4c368a15d8ade9f800cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1afc144ae889468eafe150f9f2d23505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "219bb52cb55a4ee6b62bbe71ad842007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1879cf855de407f958db716d840a6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7d96f09dfbe4c6d99fc4a5f6d3254ad",
              "IPY_MODEL_93ed2d565e334db78ecd697af6d81a91",
              "IPY_MODEL_80f6acbb5a6c43c99787f7aa815c56e5"
            ],
            "layout": "IPY_MODEL_f420101a83c54c739b69bd3860fb3723"
          }
        },
        "d7d96f09dfbe4c6d99fc4a5f6d3254ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90825c73725744faa03b4282d88ebf41",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9b9f96d2347641fa8edd757bb1de1662",
            "value": "Map:‚Äá100%"
          }
        },
        "93ed2d565e334db78ecd697af6d81a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0aa8c0ce8bf4a3d97e71497af6a0ed4",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c068be63f2974c86b9f2882a5d3edecb",
            "value": 144
          }
        },
        "80f6acbb5a6c43c99787f7aa815c56e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ca6c0151bc944d8b0d2bde1569d62ad",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0f124d28fdc94ab8b41eef5208c1f495",
            "value": "‚Äá144/144‚Äá[00:00&lt;00:00,‚Äá2888.71‚Äáexamples/s]"
          }
        },
        "f420101a83c54c739b69bd3860fb3723": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90825c73725744faa03b4282d88ebf41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b9f96d2347641fa8edd757bb1de1662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0aa8c0ce8bf4a3d97e71497af6a0ed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c068be63f2974c86b9f2882a5d3edecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ca6c0151bc944d8b0d2bde1569d62ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f124d28fdc94ab8b41eef5208c1f495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3eec9f5f98e4c0b8de8f9364cdf9da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80cf61343eb948579c28f846d543d209",
              "IPY_MODEL_bbad6be4094547878780cee2e8cc288f",
              "IPY_MODEL_cf1591a58cbc412c97f095e6cd5c8ff5"
            ],
            "layout": "IPY_MODEL_de7b7e0e15004b81ba15d57e2d58fe0f"
          }
        },
        "80cf61343eb948579c28f846d543d209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4890ec236f0e4764b3ab72125acb59df",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf96f214524d44848b2cbdfb7640c903",
            "value": "Map‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "bbad6be4094547878780cee2e8cc288f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab9215ae18d54a2b9e098d4652024f40",
            "max": 144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a3800e52e3e49ceb4f102cb547c8eee",
            "value": 144
          }
        },
        "cf1591a58cbc412c97f095e6cd5c8ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f03034c363f944f0aeb0bd2fd49dec75",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7ef18d29a0b54c859e26bd044a2fdd72",
            "value": "‚Äá144/144‚Äá[00:02&lt;00:00,‚Äá87.51‚Äáexamples/s]"
          }
        },
        "de7b7e0e15004b81ba15d57e2d58fe0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4890ec236f0e4764b3ab72125acb59df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf96f214524d44848b2cbdfb7640c903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab9215ae18d54a2b9e098d4652024f40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3800e52e3e49ceb4f102cb547c8eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f03034c363f944f0aeb0bd2fd49dec75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ef18d29a0b54c859e26bd044a2fdd72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}